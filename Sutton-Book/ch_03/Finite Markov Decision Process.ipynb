{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2595bc\"> Finite Markov Decision Processes</span>\n",
    "\n",
    "In this notebook, we will try to define the field of reinforcement learning. We would also look into the mathematical structures of a reinforcement problem such <span style=\"color:#f31818\">value functions</span> and <span style=\"color:#f31818\">Bellman equations</span>. We would also code the solution to a specific <span style=\"color:#f31818\">GridWorld</span> problem. \n",
    "\n",
    "This notebook involves a fair bit of conditional probability. You can read about conditional probability <a href=\"http://setosa.io/conditional/\">here</a>.<p>You also should be able to find out the <a href=\"https://revisionmaths.com/advanced-level-maths-revision/statistics/expectation-and-variance\">expectation </a>of a probability distribution</p>\n",
    "\n",
    "\n",
    "<a id=\"1\"></a>\n",
    "## <span style=\"color:#2595bc\"> The Agent-Environment Interface </span>\n",
    "\n",
    "Let's define some terms and then we will see an example to get a good understanding of the terms.\n",
    "\n",
    "### <span style=\"color:#2595bc\"> Definitions </span>\n",
    "\n",
    "The learner and decision-maker is called the <span style=\"color:#f31818\"><b><i>agent</i></b></span>. The thing our agent interacts with, comprising everything outside ,which <strong>cannot be manipulated by agent</strong>, is called the <span style=\"color:#f31818\"><b><i>environment</i></b></span>. \n",
    "\n",
    "These interact continually, the agent selecting actions and the environment responding to those actions and presenting new situations to the agent. The environment is often defined by it's <i>'state'</i> .The environment also gives rise to <span style=\"color:#f31818\"><b><i>rewards</i></b></span>, special numerical\n",
    "values that the agent tries to maximize over time. A complete specification of an\n",
    "environment, including how rewards are determined, defines a task , which is one instance of\n",
    "the reinforcement learning problem\n",
    "<span "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/agent_env.png\"></img>\n",
    "\n",
    "More specifically, the agent and environment interact at each of a sequence of\n",
    "discrete time steps, $t = 0, 1, 2, 3, . . .. $ . At each time step $t$, the agent receives some\n",
    "representation of the environment’s <i>state</i>, $S_t \\in \\mathbb S $, where $\\mathbb S$ is the set of possible states that the environment can have, and on that basis the agent selects an action, $A_t \\in \\mathbb A (S_t)$, where $\\mathbb A(S_t )$ is the set of actions available in state $S_t$ . One time step later, in part as a consequence of its action, the agent receives a numerical reward , $R_{t+1} \\in \\mathscr{R}$ , where $\\mathscr{R}$ is the set of possible rewards the agent might get, and finds itself in a new state,\n",
    "$S_{t+1}$ . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color:#2595bc\"> Example </span>\n",
    "One example of it can be a robot which collects garbage. Let's assume the robot has a limited battery life. It could move around and search actively for garbage, which consumes battery. It could also wait for people to come to it and deposit garbage, which doesn't consume battery. The reward for the robot would be higher when it actively searches than when people come to it. When it is low in battery it needs to head over to the base to recharge. If it tries to actively search for garbage while having low battery, it would be needed to be sent to recharge center which has a high negative reward associated with it.\n",
    "\n",
    "\n",
    "\n",
    "The agent therefore\n",
    "has three actions which make up our $\\mathbb A $. The set $\\mathbb A$ would be $\\{Search, Wait, Recharge\\}$, and its state is determined by the state of the battery. So  set $\\mathbb S$ would be $\\{High, Low\\}$.The rewards\n",
    "might be zero most of the time, but then become positive when the robot secures\n",
    "an empty can, or large and negative if the battery runs all the way down. \n",
    "\n",
    "In this example,it is <span style=\"color:#f31818\">important</span> to note that the reinforcement learning agent is <span style=\"color:#f31818\"><b><i>not</i></b></span> the entire robot. The states it\n",
    "monitors describe conditions within the robot itself,which is the battery level, not conditions of the robot’s\n",
    "external environment. The agent’s environment therefore includes the rest of the\n",
    "robot, which might contain other complex decision-making systems, as well as the\n",
    "robot’s external environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## <span style=\"color:#2595bc\"> Policy </span>\n",
    "\n",
    "This refers to how the agent selects an action $a$ at a given state $S_t$. At each time step, the agent implements a mapping from states to <b>probabilities</b>\n",
    "of selecting each possible action. This mapping is called the agent’s policy and is\n",
    "denoted $\\pi_t$, where $\\pi_t (a\\mid s)$ is the probability that $A_t = a$ if $S_t = s$\n",
    "\n",
    "Reinforcement\n",
    "learning methods specify how the agent changes its policy as a result of its experience.\n",
    "The agent’s goal, roughly speaking, is to maximize the total amount of reward it\n",
    "receives over the long run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## <span style=\"color:#2595bc\"> Goals and Rewards </span>\n",
    "\n",
    "In reinforcement learning, the Purpose or Goal of the agent is formalized in terms of a\n",
    "special reward signal passing from the environment to the agent. At each time step,\n",
    "the reward is a simple number, $R_t$. Informally, the agent’s Goal is to maximize the\n",
    "total amount of reward it receives. This means maximizing <i><b>not</b> immediate reward,\n",
    "but cumulative reward in the long run. </i>\n",
    "\n",
    "Setting the way in which our agent gets rewards is important as it would define what characteristics our agent would learn. For eg, to make a robot learn to walk, we can provide reward on each time step\n",
    "proportional to the robot’s forward motion. In making a robot learn how to escape\n",
    "from a maze, the reward is often −1 for every time step that passes prior to escape;\n",
    "this encourages the agent to escape as quickly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## <span style=\"color:#2595bc\"> Returns </span>\n",
    "We need to formally define the Goal. We introduce the concept of Returns which our agent's objective would be to maximize. \n",
    "\n",
    "Let the sequence of rewards received after time-step $t$ be $R_{t+1}, R_{t+2}, R_{t+3} . . . $\n",
    "\n",
    "In simplest case our return can be defined as the sum of rewards:\n",
    "$$ G_t = R_{t+1} + R_{t+2} + R_{t+3}+ . . + R_{T}$$\n",
    "\n",
    "where $T$ is the final time step.This approach makes sense in applications in which there\n",
    "is a natural notion of final time step, that is, when the agent–environment interaction\n",
    "breaks naturally into subsequences, which we call <i>episodes</i>, such as plays of a game,\n",
    "trips through a maze, or any sort of repeated interactions. Each episode ends in a\n",
    "special state called the terminal state, followed by a reset. Tasks with episodes of this kind are called <i>episodic tasks</i>.\n",
    "\n",
    "When the interaction does not\n",
    "break naturally into identifiable episodes, but goes on continually without limit, we call them <i>continuing tasks</i>. We cannot use the above formulation as $T$ = $\\infty$. So we introduce the concept of <span style=\"color:#f31818\"><b><i>discounting</i></b></span>. Agent now tries to select actions so that the sum of the <i>discounted rewards</i>\n",
    "it receives over the future is maximized. In particular, it chooses $A_t$ to maximize the\n",
    "expected <i>discounted return</i>:\n",
    "\n",
    "$$ G_t =  R_{t+1} + \\gamma R_{t+2} +\\gamma^2 R_{t+3}+ . . . = \\sum_{k=0}^{\\infty}R_{t+k+1}$$\n",
    "where $\\gamma$ is a parameter, $0\\leq\\gamma\\leq1$, called the <span style=\"color:#f31818\"><b>discount rate</b></span>\n",
    "\n",
    "The discount rate determines the present value of future rewards: a reward received\n",
    "$k$ time steps in the future is worth only $\\gamma^{k-1}$ times what it would be worth if it were\n",
    "received immediately. It could be justified that because of the uncertainty associated with future time steps there is a decay in the future expected rewards. The more closer we are to the time step, the more accurately we can predict/expect the reward if we take a particular action. If $\\gamma = 0$, the agent is \"myopic\" that is concerned only with the immediate rewards. It's objective is to only lean how to choose $A_t$ so as to maximize only $R_{t+1}$\n",
    "\n",
    "We also can use the newly obtained formula for episodic tasks as well, if take $\\gamma = 1$ and $R_t = 0 $ for $t > T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## <span style=\"color:#2595bc\"> Markov Property </span>\n",
    "To clarify a few things before we proceed further, by the \"state\" of the environment we mean whatever information is available to the agent through signals about the environment.\n",
    "\n",
    "Now, consider how a general environment might respond at time $t + 1$ to the action taken\n",
    "at time $t$. In the most general, causal case, this response may depend on every-\n",
    "thing that has happened earlier. In this case the dynamics can be defined only by\n",
    "specifying the complete joint probability distribution which would involve all the actions and states that were present previous to $t$ :\n",
    "$$ P_r \\Bigl\\{S_{t+1} = s^{'},R_{t+1}=r \\mid S_0, A_0, R_0. . . .S_{t-1},A_{t-1},R_t,S_t,A_t\\Bigr\\} $$\n",
    " \n",
    "\n",
    "\n",
    "If the state signal has the Markov property, on the other hand, then the\n",
    "<span style=\"color:#f31818\">environment’s response at $t + 1$ depends only on the state and action representations\n",
    "at $t$</span>, in which case the environment’s dynamics can be defined by specifying only:\n",
    "\n",
    "$$p(s^{'},r \\mid s,a) = P_r \\Bigl\\{S_{t+1}=s^{'},R_{t+1}=r \\mid S_t = s, A_t = a\\Bigr\\} $$\n",
    "\n",
    "<b>To put in simpler terms, in Markov process, if we know the current state, then we can predict the future state for an action taken, without needing the knowledge of previous passed states.</b> \n",
    "\n",
    "The Markov property is important in reinforcement learning because decisions and\n",
    "values are assumed to be a function only of the current state.\n",
    "\n",
    "One can also show that, by iterating this equation, one can predict all future\n",
    "states and expected rewards from knowledge only of the current state as well as\n",
    "would be possible given the complete history up to the current time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## <span style=\"color:#2595bc\"> Markov Decision Processes</span>\n",
    "\n",
    "A reinforcement learning task that satisfies the Markov property is called a <i>Markov\n",
    "decision process</i>, or MDP. If the state and action spaces are finite, then it is called a\n",
    "<i>finite Markov decision process</i> (<i>finite</i> MDP)\n",
    "\n",
    "Next comes the defining expression/property of MDPs which we obtain from the nature of Markov Processes. The below expression completely specify the dynamics of a finite MDP. Given any state and action $s$ and $a$, the probability\n",
    "of each possible pair of next state and reward, $s$ , $r$, is denoted\n",
    "\n",
    "## <span style=\"color:#f31818\">$$p(s',r \\mid s,a) = P_r \\Bigl\\{S_{t+1}=s',R_{t+1}=r \\mid S_t = s, A_t = a\\Bigr\\} $$<span style=\"color:#f31818\">\n",
    "\n",
    "Given the dynamics as specified by the above equation, one can compute anything else one might\n",
    "want to know about the environment, such as the expected rewards for state–action\n",
    "pairs( shows the expectation of reward,$R_{t+1}$, if we take action $a$ at state $s$ at time step $t$ ) - \n",
    "\n",
    "### $$r(s,a) = \\mathbb E\\Bigl[R_{t+1} \\mid S_{t}=s, A_{t}=a \\Bigr] = \\sum_{r \\in \\mathscr{R}}r\\sum_{s' \\in \\mathbb S} p(s',r \\mid s,a)$$ \n",
    "(are you able to reason why the double summation comes ?) <br>\n",
    "(Hint : What's the formulea for finding Expectation?)\n",
    "\n",
    "We can also find the <i> state-transition probabilities,</i> -\n",
    "### $$p(s'\\mid s,a ) = Pr\\Bigl\\{S_{t+1} = s' \\mid S_{t} = s, A_{t} = a\\Bigr\\} = \\sum_{r \\in \\mathscr{R}}p(s',r \\mid s,a)$$ \n",
    "\n",
    "and the expected rewards for (state)-(action)-(next-state) triples, \n",
    "\n",
    "### $$r(s,a, s') = \\mathbb E\\Bigl[R_{t+1} \\mid S_{t}=s, A_{t}=a \\Bigr] = \\sum_{r \\in \\mathscr{R}}r\\sum_{s' \\in \\mathbb S} p(s',r \\mid s,a)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6example\"></a>\n",
    "### <span style=\"color:#2595bc\"> Example - MDP for the robot problem</span>\n",
    "Recall the example of the trash collecting robot we had seen earlier. Let's add some more information and convert it into an MDP.\n",
    "\n",
    "Recall that the agent makes a decision at times determined by external events (or by other parts of the robot’s control system). At each such time the robot decides whether it should <ol><li>Actively search for trash</li> <li>Remain stationary and wait for\n",
    "someone to bring it trash</li> <li>Go back to home base to recharge its battery.</li></ol>\n",
    "Suppose the environment works as follows. The best way to find trash is to actively search for them, but this runs down the robot’s battery, whereas waiting does not. Whenever the robot is searching, the possibility exists that its battery will become depleted. In this case the robot must shut down and wait to be rescued (producing a low reward).\n",
    "\n",
    "The agent makes its decisions solely as a function of the energy level of the battery.\n",
    "It can distinguish two levels, high and low, so that the state set is\n",
    "$\\mathbb S = \\biggl\\{high, low\\biggr\\}$\n",
    "\n",
    "Let us call the possible decisions—the agent’s actions—<b>wait</b>, <b>search</b>, and <b>recharge</b>.\n",
    "When the energy level is high, recharging would always be foolish, so we do not\n",
    "include it in the action set for this state. The agent’s action sets are:- \n",
    "$$\\mathbb A(high) = \\Bigl\\{search, wait\\Bigr\\}$$\n",
    "$$\\mathbb A(low) = \\Bigl\\{search, wait, recharge\\Bigr\\}$$\n",
    "\n",
    "If the energy level is <b>high</b>, then a period of active search can always be completed\n",
    "without risk of depleting the battery.\n",
    "\n",
    "A period of searching that begins with a <b>high</b>\n",
    "energy level leaves the energy level high with probability $\\alpha$ and reduces it to <b>low</b>\n",
    "with probability 1 − $\\alpha$. \n",
    "\n",
    "On the other hand, a period of searching undertaken when\n",
    "the energy level is low leaves it low with probability $\\beta$ and depletes the battery with\n",
    "probability 1 − $\\beta$. In the latter case, the robot must be rescued, and the battery\n",
    "is then recharged back to high. \n",
    "\n",
    "Each trash collected by the robot counts as a unit\n",
    "reward, whereas a reward of −3 results whenever the robot has to be rescued. \n",
    "\n",
    "Let\n",
    "${r}_{search}$ and ${r}_{wait}$ , with ${r}_{search}$ > ${r}_{wait}$ , respectively denote the expected number of\n",
    "trash the robot will collect (and hence the expected reward) while searching and while\n",
    "waiting. \n",
    "\n",
    "Finally, to keep things simple, suppose that no cans can be collected during\n",
    "a run home for recharging, and that no cans can be collected on a step in which the\n",
    "battery is depleted.\n",
    "\n",
    "This system is then a finite MDP, and we can write down the\n",
    "transition probabilities and the expected rewards, as in Table below, \n",
    "<a id=\"6a\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/table.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <i>transition graph</i> is a useful way to summarize the dynamics of a finite MDP.Below is the transition graph for the recycling robot example\n",
    "<a id=\"6b\"></a>\n",
    "<img src=\"images/graph.png\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that there are two kind of nodes, <i>state nodes</i> and <i>action nodes</i>. There is a state node for each possible state( a large open circle denoted by the state name), and an action node for each state-action-state triplet (solid circle labeled by name of the action and connected by a line to the state node). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## <span style=\"color:#2595bc\"> Value Functions </span>\n",
    "\n",
    "Almost all reinforcment learning algoritms involve estimating <i>value functions</i> - functions of states ( or of state-action pairs ) that <b>estimate <i>how good</i> </b>it is for the agent to be in a given state(or to perform a action under given state). We have seen examples of value functions in the form of action-values in the previous notebook(for K-Bandit).\n",
    "\n",
    "The notion of \"how good\" is defined in terms of expected future rewards, or to be precise, expected return. As obvious, the rewards the agent can expect in future depend on what actions it will take. So, value functions are defined with respect to particular policies (policy explained in a previous <a href=\"#2\">section</a>)\n",
    "\n",
    "Recall that a policy $\\pi$ is a map from each state, $s \\in \\mathbb S$, and action, $a \\in \\mathbb A(s)$ to the probability $\\pi(a\\mid s)$ of taking $s$ when in state $s$.\n",
    "\n",
    "<img src=\"images/state_and_action.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "<img src=\"images/bellman_eqn.png\"></img>\n",
    "\n",
    "\n",
    "\n",
    "Following is the derivation of the above expressions\n",
    "\n",
    "\n",
    "<img src=\"images/Bellman.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#2595bc\">Various other forms of equation </span>\n",
    "\n",
    "Using the relations where values of state were expressed as an expression of the possible succeeding state and immediate expected rewards,we can derive other forms of equations which might be interest.\n",
    "\n",
    "###### <span style=\"color:#2595bc\"> $V_{\\pi} \\text{ in terms of } q_{\\pi}\\text{ and vice versa}$ </span>\n",
    "\n",
    "$$ v_{\\pi} = \\sum_{a \\in \\mathbb A} \\pi (a \\mid s)q_{\\pi}(s,a)$$\n",
    "$$ q_{\\pi} = R_{s}^{a} + \\gamma \\sum_{s' \\in \\mathbb S} P_{ss'}^{s}v_{\\pi}(s')$$\n",
    "\n",
    "where $R_{s}^{a}$ is the expected return after taking action $a$ from state $S$ and $P_{ss'}^{a}$ is the probability that the environment is in state $a'$ after the agent takes action $s$ from state $s$\n",
    "\n",
    "###### <span style=\"color:#2595bc\">Expectation Equations</span>\n",
    "\n",
    "<img src=\"images/v_sum.png\"></img>\n",
    "For $q_{\\pi}$\n",
    "<img src=\"images/q_sum.png\"></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "## <span style=\"color:#2595bc\"> Grid World </span>\n",
    "\n",
    "Figure shows a rectangular gridworld representation of a simple finite MDP. The cells of the grid correspond to the states of the environment. At each cell, four actions are possible: $north, south, east, and\n",
    "west$, which deterministically cause the agent to move one cell in the respective direction on the grid. \n",
    "\n",
    "Actions that would take the agent off the grid leave its location unchanged, but also result in a reward of −1. Other actions result in a reward of 0, except those that move the agent out of the special states A and B. From state A,all four actions yield a reward of +10 and take the agent to A' . From state B, all actions yield a reward of +5 and take the agent to B'. Think A as a portal .When the agent reaches A,it gets a reward of +10 and gets transported to state A'. Similar for B and B' with a reward 5.\n",
    "\n",
    "<img src=\"images/gridworld.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9a\"></a>\n",
    "### <span style=\"color:#2595bc\">Finding Policy values</span>\n",
    "\n",
    "Let's find out the policy values of each state/grid. Although there are many ways of achieveing this, right now we will find the values through Dynamic Programming method called Iterative Policy Evaluation. (More on this in the next notebook. Let's just implement as we go by and understand in the next one)\n",
    "\n",
    "We know that - \n",
    "\n",
    "$$v_{k+1} = \\sum_{a \\in \\mathbb{A}} \\pi(a \\mid s)\\Biggl(R_{s}^{a} + \\gamma\\sum_{s' \\in \\mathbb{S}}P_{ss'}^{a}v_{k}(s')\\Biggr) $$\n",
    "\n",
    "The algorithm would be -\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Set $V(s)$ randomly (say all states to 0)<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; loop untill policy good enough<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loop for $s \\in \\mathbb S$<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loop for $a \\in \\mathbb A$<br>\n",
    "\t\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$Q(s,a) = \\sum_{a \\in \\mathbb{A}} \\pi(a \\mid s)\\Biggl(R_{s}^{a} + \\gamma\\sum_{s' \\in \\mathbb{S}}P_{ss'}^{a}v_{k}(s')\\Biggr) $\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V(s):=Q(s,a)$<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;end loop<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;end loop<br>\n",
    "\n",
    "\n",
    "<a id=\"9b\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = [0,1]\n",
    "A_prime= [4,1]\n",
    "\n",
    "B = [0,3]\n",
    "B_prime = [2,3]\n",
    "\n",
    "def get_state_values(portal_initials=[A,B],\n",
    "               portal_destinations=[A_prime,B_prime],\n",
    "               portal_rewards=[10,5],\n",
    "               discount=0.9,\n",
    "               grid_size=5,\n",
    "               action_probabilities=[.25,.25,.25,.25]):\n",
    "    '''\n",
    "    grid_size --> Size of the grid world\n",
    "    portal_initials --> list of Coordinates of points/state from where the agent \n",
    "                        jumps to other states. Eg in our example, it will be [A,B]\n",
    "    portal_initials --> list of Coordinates of points/state to where the agent \n",
    "                        jumps to other states. Eg in our example, it will be [A_prime,B_prime]\n",
    "    portal_rewards ---> list of rewards when the jump occurs \n",
    "                                               Eg in our example, it will be [10,5]\n",
    "    discount ---------> parameter\n",
    "    probablities------> probabilities of picking ['L','R','U','D'] at each state\n",
    "    '''\n",
    "    gridWorld = np.zeros([grid_size,grid_size])\n",
    "    actions = ['L','R','U','D']\n",
    "    actprobs = {'L': action_probabilities[0],\n",
    "                'R': action_probabilities[1],\n",
    "                'U': action_probabilities[2],\n",
    "                'D': action_probabilities[3]}\n",
    "    \n",
    "    #Store next states for each action at all values of states\n",
    "    next_states = []\n",
    "    #Store rewards for each action at all values of states\n",
    "    action_rewards = []\n",
    "    \n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        \n",
    "        next_states.append([])\n",
    "        action_rewards.append([]) \n",
    "        \n",
    "        for j in range(grid_size):\n",
    "            next_state = dict()\n",
    "            action_reward = dict()\n",
    "            \n",
    "            if i==0:\n",
    "                next_state['U'] = [i,j]\n",
    "                action_reward['U'] = -1.\n",
    "            else:\n",
    "                next_state['U'] = [i-1,j]\n",
    "                action_reward['U'] = 0.            \n",
    "            \n",
    "            if i==grid_size-1:\n",
    "                next_state['D'] = [i,j]\n",
    "                action_reward['D'] = -1.\n",
    "            else:\n",
    "                next_state['D'] = [i+1,j]\n",
    "                action_reward['D'] = 0.            \n",
    "                        \n",
    "            if j==0:\n",
    "                next_state['L'] = [i,j]\n",
    "                action_reward['L'] = -1.\n",
    "            else:\n",
    "                next_state['L'] = [i,j-1]\n",
    "                action_reward['L'] = 0. \n",
    "                                            \n",
    "            if j==grid_size-1:\n",
    "                next_state['R'] = [i,j]\n",
    "                action_reward['R'] = -1.\n",
    "            else:\n",
    "                next_state['R'] = [i,j+1]\n",
    "                action_reward['R'] = 0. \n",
    "            \n",
    "            if [i,j] in portal_initials:\n",
    "                next_state['L']=next_state['R']=next_state['U']=next_state['D']=portal_destinations[portal_initials.index([i,j])]\n",
    "                action_reward['L']=action_reward['R']=action_reward['U']=action_reward['D']=portal_rewards[portal_initials.index([i,j])]\n",
    "            \n",
    "            next_states[i].append(next_state)\n",
    "            action_rewards[i].append(action_reward)\n",
    "\n",
    "    #Initiate V_s to zeros\n",
    "    V_s = np.zeros([grid_size,grid_size])\n",
    "    counter = 0\n",
    "    while(True):\n",
    "        Q_s = np.zeros_like(V_s)\n",
    "        for i in range(grid_size):\n",
    "            \n",
    "            for j in range(grid_size):\n",
    "                \n",
    "                for action in actions:\n",
    "                 \n",
    "                  \n",
    "                    #Find Q_(s,a) for given current V(s,a) from Bellman\n",
    "                    new_position = next_states[i][j][action]\n",
    "                    Q_s[i][j] += actprobs[action]*(action_rewards[i][j][action]+\n",
    "                                                 discount*V_s[new_position[0],new_position[1]])\n",
    "                   \n",
    "          \n",
    "         \n",
    "        if np.sum(np.abs(V_s - Q_s)) < 1e-6:\n",
    "           \n",
    "            V_s = Q_s\n",
    "            break\n",
    "        #Update\n",
    "        V_s = Q_s\n",
    "        counter +=1\n",
    "    \n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    return V_s\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAGfCAYAAACJCX/uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERGx7v8AQf3notQ+jC8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f630bd89c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "to_plot = get_state_values()\n",
    "to_plot = np.around(to_plot, decimals=2)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(to_plot)\n",
    "plt.title('')\n",
    "for (j,i),label in np.ndenumerate(to_plot):\n",
    "    plt.text(i,j,label,ha='center',va='center',color=\"white\")\n",
    "    plt.text(i,j,label,ha='center',va='center',color=\"white\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a>\n",
    "## <span style=\"color:#2595bc\"> Optimal Value Functions </span>\n",
    "\n",
    "Solving a reinforcement learning task means, roughly, finding a policy that achieves a\n",
    "lot of reward over the long run.\n",
    "\n",
    "Value functions define a partial ordering over policies. A policy $\\pi$ is defined to be better than or equal to a policy $\\pi'$ if its expected return\n",
    "is greater than or equal to that of $\\pi'$ for all states. In other words, $\\pi$ ≥ $\\pi'$ if and only\n",
    "if  $v_{\\pi}(s)$ ≥ $v_{\\pi'}(s)$ for all $s\\in \\mathbb S$.\n",
    "\n",
    "There is always at least one policy that is better than or equal to all other policies. This is an optimal policy. Although there may be more than one, we denote all the optimal policies by $\\pi_{*}$ .They share the same state-value function, called the optimal state-value function, denoted $v_{*}$ , and defined as\n",
    "\n",
    "### $$ v_{*}(s) = max_{\\pi}v_{\\pi}(s)$$ \n",
    "$\\text{for all s} \\in \\mathbb S$\n",
    "\n",
    "Optimal policies also share the same <i>optimal action-value function</i>, denoted $q_{*}$ ,\n",
    "and defined as\n",
    "\n",
    "### $$ q_{*}(s,a) = max_{\\pi}q_{\\pi}(s,a)$$ \n",
    "$\\text{for all s} \\in \\mathbb S \\text{ and for all a} \\in \\mathbb A(s)$\n",
    "\n",
    "For the state–action pair $(s, a)$, this function gives the\n",
    "expected return for taking action $a$ in state $s$ and thereafter following an optimal\n",
    "policy. Thus, we can write $q_{∗}$ in terms of $v_{∗}$ as follows:\n",
    "\n",
    "### $$q_{*}(s,a) = \\mathbb E\\Bigl[R_{t+1} + \\gamma v_{*}(S_{t+1} \\mid S_{t}=s,A_{t}=a\\Bigr]$$\n",
    "\n",
    "As, the optimal functions also we follow Bellman's equation, \n",
    "\n",
    "<img src=\"images/optimal.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"10a\"></a>\n",
    "Therefore, to get the optimal policy, instead of taking the expectation, as we were taking, earlier, we would have to take the branch with maximum value. \n",
    "\n",
    "The iterative algorithm for this would be of the form - \n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Set $V(s)$ randomly (say all states to 0)<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; loop untill policy good enough<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loop for $s \\in \\mathbb S$<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loop for $a \\in \\mathbb A$<br>\n",
    "\t\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$Q(s,a) =max_{a}\\Biggl(R_{s}^{a} + \\gamma\\sum_{s' \\in \\mathbb{S}}P_{ss'}^{a}v_{k}(s')\\Biggr) $\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V(s):=Q(s,a)$<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;end loop<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;end loop<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_policy(portal_initials=[A,B],\n",
    "               portal_destinations=[A_prime,B_prime],\n",
    "               portal_rewards=[10,5],\n",
    "               discount=0.9,\n",
    "               grid_size=5,\n",
    "               action_probabilities=[.25,.25,.25,.25]):\n",
    "    '''\n",
    "    grid_size --> Size of the grid world\n",
    "    portal_initials --> list of Coordinates of points/state from where the agent \n",
    "                        jumps to other states. Eg in our example, it will be [A,B]\n",
    "    portal_initials --> list of Coordinates of points/state to where the agent \n",
    "                        jumps to other states. Eg in our example, it will be [A_prime,B_prime]\n",
    "    portal_rewards ---> list of rewards when the jump occurs \n",
    "                                               Eg in our example, it will be [10,5]\n",
    "    discount ---------> parameter\n",
    "    probablities------> probabilities of picking ['L','R','U','D'] at each state\n",
    "    '''\n",
    "    gridWorld = np.zeros([grid_size,grid_size])\n",
    "    actions = ['L','R','U','D']\n",
    "    actprobs = {'L': action_probabilities[0],\n",
    "                'R': action_probabilities[1],\n",
    "                'U': action_probabilities[2],\n",
    "                'D': action_probabilities[3]}\n",
    "    \n",
    "    #Store rewards for each action at all values of states\n",
    "\n",
    "    #init_rewards = {'L':0,'R':0,'U':0,'D':0}\n",
    "    \n",
    "    next_states = []\n",
    "    action_rewards = []\n",
    "    \n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        \n",
    "        next_states.append([])\n",
    "        action_rewards.append([]) \n",
    "        \n",
    "        for j in range(grid_size):\n",
    "            next_state = dict()\n",
    "            action_reward = dict()\n",
    "            \n",
    "            if i==0:\n",
    "                next_state['U'] = [i,j]\n",
    "                action_reward['U'] = -1.\n",
    "            else:\n",
    "                next_state['U'] = [i-1,j]\n",
    "                action_reward['U'] = 0.            \n",
    "            \n",
    "            if i==grid_size-1:\n",
    "                next_state['D'] = [i,j]\n",
    "                action_reward['D'] = -1.\n",
    "            else:\n",
    "                next_state['D'] = [i+1,j]\n",
    "                action_reward['D'] = 0.            \n",
    "                        \n",
    "            if j==0:\n",
    "                next_state['L'] = [i,j]\n",
    "                action_reward['L'] = -1.\n",
    "            else:\n",
    "                next_state['L'] = [i,j-1]\n",
    "                action_reward['L'] = 0. \n",
    "                                            \n",
    "            if j==grid_size-1:\n",
    "                next_state['R'] = [i,j]\n",
    "                action_reward['R'] = -1.\n",
    "            else:\n",
    "                next_state['R'] = [i,j+1]\n",
    "                action_reward['R'] = 0. \n",
    "            \n",
    "            if [i,j] in portal_initials:\n",
    "                next_state['L']=next_state['R']=next_state['U']=next_state['D']=portal_destinations[portal_initials.index([i,j])]\n",
    "                action_reward['L']=action_reward['R']=action_reward['U']=action_reward['D']=portal_rewards[portal_initials.index([i,j])]\n",
    "            \n",
    "            next_states[i].append(next_state)\n",
    "            action_rewards[i].append(action_reward)\n",
    "\n",
    "    V_s = np.zeros([grid_size,grid_size])\n",
    "    counter = 0\n",
    "    while(True):\n",
    "        Q_s = np.zeros_like(V_s)\n",
    "        for i in range(grid_size):\n",
    "            \n",
    "            for j in range(grid_size):\n",
    "                \n",
    "                to_compare = []\n",
    "                for action in actions:\n",
    "                 \n",
    "                    #Find Q_(s,a) for given current V(s,a) from Bellman and optimal condition\n",
    "                    new_position = next_states[i][j][action]\n",
    "                    var = (action_rewards[i][j][action]+\n",
    "                                                 discount*V_s[new_position[0],new_position[1]])\n",
    "                    \n",
    "                    to_compare.append(var)\n",
    "                Q_s[i][j] = np.max(to_compare)\n",
    "        if np.sum(np.abs(V_s - Q_s)) < 1e-5:\n",
    "            \n",
    "            V_s = Q_s\n",
    "            break\n",
    "        V_s = Q_s\n",
    "        counter +=1\n",
    "    return V_s\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAGfCAYAAACJCX/uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f62d7910ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot the optimal state function values\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "to_plot = get_optimal_policy()\n",
    "to_plot = np.around(to_plot, decimals=2)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(to_plot)\n",
    "for (j,i),label in np.ndenumerate(to_plot):\n",
    "    plt.text(i,j,label,ha='center',va='center',color=\"white\")\n",
    "    plt.text(i,j,label,ha='center',va='center',color=\"white\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
